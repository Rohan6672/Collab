# -*- coding: utf-8 -*-
"""Week 4

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VNaaA705PM0WljkfaTHEdX4D4dM_e2q5
"""

import os
import numpy as np
import matplotlib.pyplot as plt
from tensorflow import keras
from tensorflow.keras import Model
from tensorflow.keras.layers import Conv2D, \
    Conv2DTranspose, MaxPooling2D, Dropout, \
    BatchNormalization, Activation, concatenate
from tensorflow.keras.optimizers import Adam
from tensorflow.data import Dataset
from tensorflow.data import AUTOTUNE
from tensorflow import TensorSpec
from tensorflow import dtypes
from PIL import Image
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping

# Commented out IPython magic to ensure Python compatibility.
# Connect to Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Create a Project Directory
PROJECT_DIR = "/content/drive/MyDrive/camvid-semantic-segmentation"
if not os.path.exists(PROJECT_DIR):
#     %mkdir {PROJECT_DIR}
# %cd {PROJECT_DIR}

DATA_DIR = './data/CamVid'

# load repo with data if it doesn't exist
if not os.path.exists(DATA_DIR):
    print('Loading data...')
    os.system('git clone https://github.com/alexgkendall/SegNet-Tutorial ./data')
    print('Done!')

x_train_dir= os.path.join(DATA_DIR,'train')
y_train_dir= os.path.join(DATA_DIR,'trainannot')

x_valid_dir= os.path.join(DATA_DIR,'val')
y_valid_dir=os.path.join(DATA_DIR,'valannot')

x_test_dir=os.path.join(DATA_DIR,'test')
y_test_dir= os.path.join(DATA_DIR,'testannot')

# open a given image
def open_image(filename, num_channels=3):
    # open the given image
    with Image.open(filename) as img:
        if num_channels == 3:
            # convert to rgb
            img = img.convert("RGB")
        # return numpy array of image
        return np.asarray(img)

# crop a numpy array
def crop_data(height, width, image):
    cropped_data = image[:height, :width, :]
    return cropped_data

def visualize(image_index, image, mask=None, pred_mask=None):
    # Set plot width to 5, 10 or 15 depending on
    # if a mask and/or predicted mask were given
    plot_width = 5
    if mask is not None:
        plot_width +=5
    if pred_mask is not None:
        plot_width += 5

    # Create a figure
    fig = plt.figure(figsize=(plot_width,3))

    # Add the image to the figure
    ax = fig.add_subplot(131)
    ax.imshow(image)
    ax.set_title("Image: %d" %(image_index,))

    # Add the mask to the figure
    if mask is not None:
        ax = fig.add_subplot(132)
        color_ax = ax.matshow(np.argmax(mask, axis=-1))
        ax.set_title("Ground Truth Mask")
        ax.xaxis.tick_bottom()

    # Add the predicted mask to the figure if it exists
    if pred_mask is not None:
        ax = fig.add_subplot(131)
        color_ax = ax.matshow(np.argmax(pred_mask.squeeze(), axis=-1))
        ax.set_title("Predicted Mask")
        ax.xaxis.tick_bottom()

    # Create a color bar
    fig.colorbar(color_ax)
    plt.show()

# class CamVidDataset(list):
#   """CamVid Dataset. Read images, apply augmentation
#   and preprocessing transformations.
#   Args:
#       images_dir (str): path to images folder
#       masks_dir (str): path to segmentation masks folder
#       class_values (list): values of classes to extract
#                             from segmentation mask
#   """

#   CLASSES = ['sky', 'building', 'pole', 'road', 'pavement',
#               'tree', 'signsymbol', 'fence', 'car',
#               'pedestrian', 'bicyclist', 'unlabelled']


#   def __init__(
#       self,
#       images_dir,
#       masks_dir,
#       classes=CLASSES,
#       height=352,
#       width=480,
#       batch_size=8,
#       shuffle=True
#   ):

#     self.ids=os.listdir(images_dir)

#     self.images_fps=[os.path.join(images_dir,image_id)for image_id in self.ids]
#     self.masks_fps= [os.path.join(masks_dir, image_id) for image_id in self.ids]
#     self.class_values = [self.CLASSES.index(cls.lower())for cls in classes]

#     self.height= height
#     self.width = width
#     self.batch_size = batch_size
#     self.shuffle = shuffle
#     self.indices = np.arange(len(self.ids))

#   def __getitem_(self,i):
#     image= open_image(self.images_fps[i],3)
#     mask= open_image(self.masks_fps[i],0)

#     masks= [(mask==v)for v in self.class_values]

#     mask = np.stack(masks,axis=-1).astype('float')

#     if mask.shape[-1] != 1:
#       background = 1-mask.sum(axis=-1,keepdims=True)
#       mask = np.concatenate((mask,background),axis=-1)

#     image= crop_data(self.height,self.width,image)
#     mask= crop_data(self.height,self.width,mask)

#     return image, mask

#   def __len__(self):
#     return len(self.ids)

#   def generate(self):
#     if self.shuffle:
#       np.random.shuffle(self.indices)

#       for index in self.indices:
#         yield self[index]

#   def get_generator(self):
#     tfdatagen = Dataset.from_generator(
  #     self.generate,


  #     output_signature=(
              
  #             TensorSpec((self.height,self.width,3),dtypes.float32),
  #             TensorSpec((self.height,self.width,len(self.class_values)+1),dtypes.float32))

  #         )
      
  #     return (tfdatagen
  #             .batch(self.batch_size)
  #             .prefetch(AUTOTUNE))


# classes for data loading and preprocessing
class CamVidDataset(list):
    CLASSES = ['sky', 'building', 'pole', 'road', 'pavement',
               'tree', 'signsymbol', 'fence', 'car',
               'pedestrian', 'bicyclist', 'unlabelled']
    def __init__(
            self,
            images_dir,
            masks_dir,
            classes=CLASSES,
            height=352,
            width=480,
            batch_size=8,
            shuffle=True
    ):
        # get the list of all image ids
        self.ids = os.listdir(images_dir)
        # get the paths of the images and the masks
        self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ids]
        self.masks_fps = [os.path.join(masks_dir, image_id) for image_id in self.ids]
        # convert str names to class values on masks
        self.class_values = [self.CLASSES.index(cls.lower()) for cls in classes]
        # save height, width, batch size, and shuffle
        self.height = height
        self.width = width
        self.batch_size = batch_size
        self.shuffle = shuffle
        # create list of indices for generator to loop through
        self.indices = np.arange(len(self.ids))
    def __getitem__(self, i):
        # read the image and mask from disk
        image = open_image(self.images_fps[i], 3)
        mask = open_image(self.masks_fps[i], 0)
        # extract certain classes from mask (e.g. cars)
        # create a list of one-hot encoded mask for each of the clases in class_values
        masks = [(mask == v) for v in self.class_values]
        # convert to np array from list of masks
        mask = np.stack(masks, axis=-1).astype('float')
        # add background if mask is not binary
        if mask.shape[-1] != 1:
            # encode background: 0 if any class in each pixel, else 1
            # since the classes are one-hot encoded, we can use background = 1 - sum
            background = 1 - mask.sum(axis=-1, keepdims=True)
            # add the background mask to the stack of one-hot encoded masks
            mask = np.concatenate((mask, background), axis=-1)
        # Crop image and mask
        image = crop_data(self.height, self.width, image)
        mask = crop_data(self.height, self.width, mask)
        return image, mask
    def generate(self):
         # Shuffle indices
         if self.shuffle:
             np.random.shuffle(self.indices)
         # for iterator calls __getitem__ to
         # return a pair of image and mask
         for index in self.indices:
             yield self[index]
    def get_generator(self):
        tfdatagen = Dataset.from_generator(
            # create a tf.Dataset generator using the generate method
            self.generate,
            # define the outputs of the tf.Dataset generator
            output_signature=(
                # image shape and datatype
                TensorSpec((self.height, self.width, 3),
                           dtypes.float32),
                # mask shape and datatype
                TensorSpec((self.height, self.width, len(self.class_values) + 1),
                           dtypes.float32))
        )
        return (tfdatagen.batch(self.batch_size).prefetch(AUTOTUNE))

    def __len__(self):
        return len(self.ids)

CLASSES=['car','pedestrian','sky','tree','road','building']


dataset=CamVidDataset(x_train_dir,y_train_dir,classes=CLASSES)
print(x_train_dir)
print(len(dataset))
image,mask = dataset[0]
print(f'Image shape:{image.shape}')
print(f'Mask shape:{mask.shape}')

image,mask = dataset[5]
visualize(5,image,mask)

for i, the_class in enumerate(CLASSES):
  print(i, the_class)

BATCH_SIZE=8

INPUT_HEIGHT= 352
INPUT_WIDTH = 480

train_dataset = CamVidDataset(
    
    x_train_dir,
    y_train_dir,
    classes=CLASSES,
    height=INPUT_HEIGHT,
    width= INPUT_WIDTH,
    batch_size=BATCH_SIZE

)

valid_dataset = CamVidDataset(
    x_valid_dir,
    y_valid_dir,
    classes=CLASSES,
    height=INPUT_HEIGHT,
    width=INPUT_WIDTH,
    batch_size=BATCH_SIZE
)

train_generator = train_dataset.get_generator()
valid_generator= valid_dataset.get_generator()

def conv2d_block(input_layer, n_filters, kernel_size=3,batchnorm=True):
  x = input_layer
  for i in range(2):
    x= Conv2D(filters=n_filters,kernel_size=(kernel_size,kernel_size),kernel_initializer="he_normal", padding="same")(x)
    if batchnorm:
      x=BatchNormalization()(x)
    x=Activation("relu")(x)
  return x

# build the U-net model
def get_unet (input_layer, n_filters=16, dropout=0.5, batchnorm=True,use_transpose=True):
  x = input_layer
  # convs to keep track of the conv blocks
  convs = []
  # contracting path, apply a series of 4 convolution blocks
  for i in range(4) :
    # apply a Conv2D block
    x = conv2d_block(x, n_filters=n_filters*(2**i),
      kernel_size=3, batchnorm=batchnorm)
    convs.append(x)
    # down scale the tensor for every 2x2 into 1x1
    x = MaxPooling2D((2,2))(x)
    
    x = Dropout(dropout*(1,.5) [i==0])(x)
    x = conv2d_block(x, n_filters=n_filters* (2**4) ,  kernel_size=3, batchnorm=batchnorm)

  # Expanding path
  for i in range(3,-1,-1): 
    if use_transpose:
      x = Conv2DTranspose(n_filters*(2**i) , (3,3),strides=(2,2), padding='same')(x)
    else:
      x = Resizing(2*x.shape[1], 2*x. shape[2]) (x)
    # add skip connection
    x= concatenate([x, convs[i]])
    # apply dropout to avoid overfitting
    x= Dropout(dropout)(x)
    # apply a Conv2D block
    x = conv2d_block(x, n_filters=n_filters*(2**1) ,kernel_size=3, batchnorm=batchnorm)
    outputs= Conv2D(n_filters, (1, 1), activation='softmax')(x)
    model = Model(inputs=[input_layer], outputs=[outputs])
  return model

N_CHANNELS= 3
input_img = keras.Input((INPUT_HEIGHT, INPUT_WIDTH, N_CHANNELS), name='img')
model = get_unet(input_img, n_filters=len(CLASSES) + 1, dropout=0.05, batchnorm=True, use_transpose=True)
model.compile(optimizer=Adam(),loss="binary_crossentropy", metrics=["accuracy"])
model.summary()

from matplotlib.cbook import flatten

EPOCHS = 100 # change this to what you want
# Train model on dataset
#ModelCheckpoint will automatically save the best model
#EarlyStopping will automatically stop when it stops improving
history = model.fit(x=train_generator, validation_data=valid_generator, epochs=EPOCHS, callbacks=[ModelCheckpoint(filepath='./ckpt.hdf5', monitor='val_accuracy', save_best_only=True, verbose=1), EarlyStopping(monitor='val_accuracy', patience=10,verbose=1)])

plt.subplot(122)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train','Test'],loc='upper left')
plt.show

model.save_weights("ckpt.hdf5")

model.load_weights("ckpt.hdf5")

test_dataset=CamVidDataset(
    x_test_dir,
    y_test_dir,
    classes=CLASSES,
    height=INPUT_HEIGHT,
    width=INPUT_WIDTH,
    batch_size=BATCH_SIZE,
    shuffle=False
)
test_generator= test_dataset.get_generator()

indices=[56,79,87,71,87]

for index in indices:
  image, gt_mask= test_dataset[index]
  image_array= np.expand_dims(image,axis=0).astype('float')
  pr_mask = model.predict(image_array)

  visualize(index,image,gt_mask,pr_mask)

for i, the_class in enumerate(CLASSES):
  print(i, the_class)

def visualize_pred(pred_mask):
    # Set plot width to 5, 10 or 15 depending on
    # if a mask and/or predicted mask were given
    plot_width = 5
    # if mask is not None:
    #     plot_width +=5
    # if pred_mask is not None:
    #     plot_width += 5

    # Create a figure
    fig = plt.figure(figsize=(8,6))
    # fig = plt.figure()

    ax = fig.add_subplot(111)
    color_ax = ax.matshow(np.argmax(pred_mask.squeeze(), axis=-1))
    ax.set_title("Predicted Mask")
    ax.xaxis.tick_bottom()

    # Create a color bar
    fig.colorbar(color_ax)
    plt.show()

image=open_image('/content/drive/MyDrive/camvid-semantic-segmentation/data/CamVid/test/0001TP_008850.png',3)
cropped_image= crop_data(INPUT_HEIGHT,INPUT_WIDTH,image)

image_array= np.expand_dims(
    cropped_image,axis=0).astype('float')

pr_mask=model.predict(image_array)
visualize_pred(pr_mask)

for i, the_class in enumerate(CLASSES):
  print(i , the_class)

y_hat=model.predict(test_generator)
y_hat= np.argmax(y_hat, axis =-1)
y_hat= y_hat.flatten()

y_real = np.array([item for batch in test_generator for item in batch[1]])
y_real = np.argmax(y_real, axis=-1)
y_real= y_real.flatten()

from sklearn.metrics import confusion_matrix, plot_confusion_matrix
import pandas as pd
import seaborn as sns

df_cm= pd.DataFrame(confusion_matrix(y_real,y_hat, normalize="true"),index=CLASSES+["Background"],columns=CLASSES+["Background"])
plt.figure(figsize=(10,7))
sns.heatmap(df_cm, annot=True)

plt.xlabel("Predicted label")
plt.ylabel("True label")
plt.show()