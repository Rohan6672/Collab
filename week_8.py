# -*- coding: utf-8 -*-
"""Week 8

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iJ9ysBCsnW92qkdLuoEoBn0D4pC7pvwa
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/drive/')

# %cd /content # cd to the folder that you used for Week 7

!wget http://images.cocodataset.org/zips/val2017.zip
!unzip val2017.zip
!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip
!unzip annotations_trainval2017.zip

# Clone the detr github repo
!git clone https://github.com/facebookresearch/detr.git
import os
os.chdir('detr')
!git checkout 8a144f83a287f4d3fece4acdf073f387c5af387d

!pip install --upgrade --no-cache-dir gdown
!gdown --id 1nKot1S6ARpB6wnyx3R1tefduUiUIjO9p
!gdown --id 1rkUK6OG7js5gYt1bhCUanPNqmuxwkVZX
!unzip -q WIDER_train.zip -d ./data
!unzip -q WIDER_val.zip -d ./data

!wget http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/support/bbx_annotation/wider_face_split.zip
!unzip -q wider_face_split.zip -d ./data

!pip install imagesize

import os
import sys
import json
import numpy as np
import shutil
from tqdm.notebook import tqdm
import imagesize
from pathlib import Path


# Convert Widerface annotations txt file to COCO annotations JSON file 
def widerface2coco(outputpath, image_root, json_name, annopath, coco_template_file_path):
	  # Open and load the COCO JSON Template
		with open(coco_template_file_path, 'r') as f:
				dataset = json.load(f)
		
		# Start Progress bar
		pbar = tqdm(total=os.path.getsize(annopath), unit='b')

		with open(annopath,'r') as f:
			# Initialize counters for annotaiton ids
			img_id=1
			anno_id=1

			line = f.readline().strip()
			pbar.update(f.tell() - pbar.n)

			# Iterate through text file until we reach end
			# End of file is signified with an empty line
			while len(line) > 1:
				# Check if current line is an imagae path
				if '--' in line:
					# Convert Image Annotation
					width, height = imagesize.get(os.path.join(image_root, line))
					dataset["images"].append({"file_name": line, "coco_url": "local", "height": height, "width": width, "flickr_url": "local", "id": img_id})
					
					# Get number of bounding box annotations
					line = f.readline().strip()
					pbar.update(f.tell() - pbar.n)
					
					# Convert all bounding box annotations
					for gt in range(int(line)):
						line = f.readline().strip()
						pbar.update(f.tell() - pbar.n)
						# Convert annotation into int list and take the first four, other labels are ignored
						x1,y1,wid,hei=list(map(int, line.split()))[:4]
						dataset["annotations"].append({"segmentation": [], "iscrowd": 0, "area": wid*hei, "image_id": img_id, "bbox": [x1, y1, wid, hei], "category_id": 1, "id": anno_id})
						anno_id += 1

					# Update image id 
					img_id += 1
				
				# Read next line
				line = f.readline().strip()
				pbar.update(f.tell() - pbar.n)

		# Close Bar
		pbar.close()

		# Dump COCO annotations into JSON file
		json_name = os.path.join(outputpath, "{}.json".format(json_name))

		with open(json_name, 'w') as f:
			json.dump(dataset, f)

coco_template_file_path = '/content/coco_json_template.json'

outputpath="./data/"
image_root_val='./data/WIDER_val/images/'
json_name_val="WIDERFaceValCOCO"
annopath_val='./data/wider_face_split/wider_face_val_bbx_gt.txt'
image_root_train='./data/WIDER_train/images/'
json_name_train="WIDERFaceTrainCOCO"
annopath_train='./data/wider_face_split/wider_face_train_bbx_gt.txt'


widerface2coco(outputpath, image_root_train, json_name_train, annopath_train, coco_template_file_path)
widerface2coco(outputpath, image_root_val, json_name_val, annopath_val, coco_template_file_path)

outputpath = "./data/"
# Validation set
image_root_val = './data/WIDER_val/images/'
json_name_val = "WIDERFaceValCOCO"
annopath_val = './data/wider_face_split/wider_face_val_bbx_gt.txt'
# Training set
image_root_train = './data/WIDER_train/images/'
json_name_train = "WIDERFaceTrainCOCO"
annopath_train = './data/wider_face_split/wider_face_train_bbx_gt.txt'

widerface2coco(outputpath, image_root_val, json_name_val, annopath_val, coco_template_file_path)
widerface2coco(outputpath, image_root_train, json_name_train, annopath_train, coco_template_file_path)

with open('./data/WIDERFaceTrainCOCO.json','r')as fp:
    train_json=json.load(fp)

print(len(train_json['images']))
print(len(train_json['annotations']))

def build(image_set,args):
  root=Path(args.coco_path)
  assert root.exists(),f'provided COCO path{root}does not exist'
  mode='instances'
  PATHS={
      "train":(root/"WIDER_train/images",root/'WIDERFaceTrainCOCO.json'),
      "val":(root/"WIDER_val/images",root/'WIDERFaceValCOCO.json'),
      
  }

import os
os.getcwd()

!python main.py --coco_path '/content/detr/data' --epochs 2 --batch_size 3 --resume 'https://dl.fbaipublicfiles.com/detr/detr-r50-e632da11.pth' --output_dir './ckpts'

# Clone the detr github repo
!git clone https://github.com/facebookresearch/detr.git
import os
os.chdir('detr')
!git checkout 8a144f83a287f4d3fece4acdf073f387c5af387d

# Import required libraries
import argparse
import random
from pathlib import Path

import numpy as np
import torch
import torchvision.transforms as T
import matplotlib.pyplot as plt
import PIL.Image

import util.misc as utils
from models import build_model

from main import get_args_parser

from argparse import ArgumentParser
from main import get_args_parser
import PIL

parser=ArgumentParser(description='DETR args parser',parents=[get_args_parser()])
args=parser.parse_args(args=[])
#args.resume='ckpts/checkpoint.pth'
args.resume = 'https://dl.fbaipublicfiles.com/detr/detr-r50-e632da11.pth'
args.device='cpu'

if args.output_dir:
    Path(args.output_dir).mkdir(parents=True,exist_ok=True)
args.distributed=False

print(args)

model,criterion,postprocessors = build_model(args)

device = torch.device(args.device)
model.to(device)

# COCO Classes
CLASSES = [
   'N/A', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',
   'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A',
   'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse',
   'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack',
   'umbrella', 'N/A', 'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis',
   'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',
   'skateboard', 'surfboard', 'tennis racket', 'bottle', 'N/A', 'wine glass',
   'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich',
   'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',
   'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table', 'N/A',
   'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard',
   'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A',
   'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',
   'toothbrush'
]

# Colors for visualization
COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],
          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]

# Standard PyTorch mean-std input image normalization
transform = T.Compose([
    T.ToTensor(),
    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

# For output bounding box post-processing
def box_cxcywh_to_xyxy(x):
    x_c, y_c, w, h = x.unbind(1)
    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),
         (x_c + 0.5 * w), (y_c + 0.5 * h)]
    return torch.stack(b, dim=1)

# Resize bounding boxes to be full size of the image
def rescale_bboxes(out_bbox, size):
    img_w, img_h = size
    b = box_cxcywh_to_xyxy(out_bbox)
    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)
    return b

def detect(im,model,transform):

    img=transform(im).unsqueeze(0)

    assert img.shape[-2]<=1600 and img.shape[-1]<=1600,'Demo model only supports images up to 1600 on each side.'

    outputs= model(img)

    probas = outputs['pred_logits'].softmax(-1)[0,:,:-1]
    keep= probas.max(-1).values>0.7

    bboxes_scaled = rescale_bboxes(outputs['pred_boxes'][0,keep],im.size)

    return probas[keep], bboxes_scaled

def plot_results(pil_img, class_probs, b_boxes, classes, is_ground_truth=False):
    plt.figure(figsize=(16,10))
    plt.axis('off')
    ax = plt.gca()
    ax.imshow(pil_img)
    
    for p, (xmin, ymin, xmax, ymax), c in zip(class_probs, b_boxes, COLORS * 100):
        cl = p if is_ground_truth else p.argmax()
        
        # If the class isn't present, skip this annotation
        if CLASSES[cl] not in classes:
            continue
        # Plot bounding box and label (Note difference in bounding box format, xmax vs xmax-xmin)
        xmax = xmax if is_ground_truth else xmax-xmin
        ymax = ymax if is_ground_truth else ymax-ymin
        ax.add_patch(plt.Rectangle((xmin, ymin), xmax, ymax, fill=False, color=c, linewidth=3))

        text = f"{CLASSES[cl]}" if is_ground_truth else f"{CLASSES[cl]}: {p[cl]:0.2f}"
        ax.text(xmin,ymin, text, fontsize=15, bbox=dict(facecolor="yellow", alpha=0.5))

    # Show the plot
    plt.show()

import json 
annFile = '/content/annotations/instances_val2017.json'

with open(annFile,'r') as json_file:
  anno_dict=json.load(json_file)

annos=anno_dict["annotations"]
images=anno_dict["images"]

rand_index=np.random.randint(0,len(images))

image_id=images[rand_index]['id']

file_name=images[rand_index]['file_name']

annos_in_img=[(anno['category_id'],anno['bbox'])for anno in annos if anno['image_id']==image_id]

the_image = PIL.Image.open('/content/detr/detr/FireImage1.webp').convert('RGB').resize((800,400))

!pwd

scores, boxes = detect(the_image,model,transform)
plot_classes=['person']

print('Predicted:')
plot_results(the_image,scores,boxes.tolist(),CLASSES)

print('Ground Truth')

